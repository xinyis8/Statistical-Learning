---
title: "KNN and Linear"
author: "Name: Xinyi Song; NetId:xinyis8"
date: "9/8/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Question 1

a. Write a function myknn(xtest, xtrain, ytrain, k) that fits a KNN model that predict a target point or multiple target points xtest. Here xtrain is the training dataset covariate value,ytrain is the training data outcome, and k is the number of nearest neighbors. Use the L2 norm to evaluate the distance between two points. Please note that you cannot use any additional R package within this function.

`Solution`

```{r}
# Question 1 
# myknn function Question 1.a
myknn = function(xtest,xtrain,ytrain,k){
  y_hat = matrix(0,nrow(xtest),1)
  for (i in 1:nrow(xtest)){
    d=matrix(0,nrow(xtrain),1)
    for (j in 1:nrow(xtrain)){
      d[j] = sqrt(sum((xtest[i,]-xtrain[j,])^2))
    }
    d_index =  order(d)[1:k]
    y_hat[i] = mean(ytrain[d_index])
  }
  return(y_hat)
}
```

b. Generate 1000 observations from a five-dimensional normally distribution:

$$
N\sim(\mu,\Sigma_{5\times5})
$$
where $\mu = (1, 2, 3, 4, 5)^{\mathrm{T}}$ and$\Sigma_{5\times5}$ is an autoregressive covariance matrix, with the (i,j)th entry equal to $0.5^{|i-j|}$. Then generate the outcome values Y based on the linear model 

$$
Y=X_{1}+X_{2}+(X_{3}-2.5)^{2}+\epsilon
$$
where $\epsilon$ follows i.i.d. standard normal distribution. Use set.seed(1) right before you generate this entire data. Print the first 3 entries of your data.

`Solution`

```{r}
library(MASS)
n=1000
p=5
set.seed(1)
Sigma = matrix(0,5,5)
for (i in 1:5){
  for (j in 1:5){
Sigma[i,j] = 0.5^(abs(i-j))
  }
}
Z = mvrnorm(n, mu=c(1,2,3,4,5), Sigma=Sigma) 
e = rnorm(1000,0,1)
Y = Z[,1]+Z[,2]+(Z[,3]-2.5)^2+e
# Print the first three data of Y
print(Y[1:3])
```

The first 3 entries of your data is  4.135994 5.365376 5.505069.

c. Use the first 400 observations of your data as the training data and the rest as testing data.Predict the Y values using your KNN function with k = 5. Evaluate the prediction accuracy using mean squared error.

$$
\frac{1}{N}\sum_{i}(y_{i}-\hat{y_{i}})^2
$$

`Solution`

```{r}
# Question 1.c   MSE when k = 5
Y_train<- Y[1:400]
Xtrain <- Z[1:400,]
Xtest <- Z[401:1000,]
y_true = Y[401:1000]
y_predict = myknn(Xtest,Xtrain,Y_train,5)
MSE <- sum((y_predict-y_true)^2)/nrow(y_predict)
print(MSE) 
```
when k=5, the mean square error is 2.191387

d. Compare the prediction error of a linear model with your KNN model. Consider k being 1,
2,3,...,9,10,15, 20,..., 95, 100. Demonstrate all results in a single, easily interpretable figure with proper legends.


```{r}

# Question 1.d
MSE = matrix(0,100,1)
for (k in 1:100){
  Y_train<- Y[1:400]
  Xtrain <- Z[1:400,]
  Xtest <- Z[401:1000,]
  y_true = Y[401:1000]
  y_predict = myknn(Xtest,Xtrain,Y_train,k)
  MSE[k] <- sum((y_predict-y_true)^2)/nrow(y_predict)
}
datawhole <-as.data.frame(cbind(MSE,seq(1,100,1)))
colnames(datawhole)<-c("KNN_MSE","K")
lm<-lm(Y_train~Xtrain)
Ytest_pred_lm=lm$coef[1]+lm$coef[2]*Xtest[,1]+lm$coef[3]*Xtest[,2]+lm$coef[4]*Xtest[,3]+lm$coef[5]*Xtest[,4]+lm$coef[6]*Xtest[,5]
datawhole$LS_MSE=sum((Ytest_pred_lm-y_true)^2)/length(Ytest_pred_lm)
# plot
library(ggplot2)
library(reshape2)
data.graph<-melt(datawhole,id.vars="K")
ggplot(data=data.graph,aes(x=K,y=value,col=variable))+
  geom_point(size=1)+
  xlab('K')+ylab('MSE') +
  scale_colour_manual(name = 'Model',values=c('black','red'),labels = c('KNN_MSE','LS_MSE'))+
  labs(title="MSE")+
  theme(plot.title=element_text(colour="black",size=10, hjust = 0.5))
```

Based on the output above, we can see that the under certain k, knn's MSE is much smaller than that of linear regression and performs better. 

As the k increases in certain range, the MSE decreases and after some point, the MSE increases as k increases, which indicates overfitting. 

# Question 2

a. Based on this description, write your own R function mylm_g(x, y, delta, epsilon,
maxitr) to implement this optimization version of linear regression. The output of this function should be a vector of the estimated beta value.

`Solution`

```{r}
mylm_g=function(x,y, beta, delta, epsilon,maxitr){
  grad = matrix(0,ncol(x),1)
  while(k<=maxitr){
  #for (i in 1:nrow(x)){
      #grad = grad + as.matrix(x[i,])%*%(as.matrix(y[i])-(t(as.matrix(x[i,])))%*%beta)
     #}
    grad<-(-1)/nrow(x)*t(x)%*%(y-x%*%beta)
     if(sqrt(sum((delta*grad)^2))>epsilon){
      beta<-beta-delta*grad
    }
    else {
      break
    }
    k = k+1
  }
  return(beta)
}
```

b. Test this function on the Boston Housing data from the mlbench package. Documentation
is provided here if you need a description of the data. We will remove medv, town and tract from the data and use cmedv as the outcome. We will use a scaled and centered version of the data for estimation. Please also note that in this case, you do not need the intercept term. And you should compare your result to the lm() function on the same data. Experiment on different maxitr values to obtain a good solution. However your function should not run more than a few seconds.

`Solution`

```{r}
library(mlbench)
data(BostonHousing2)
X = BostonHousing2[, !(colnames(BostonHousing2) %in% c("medv", "town", "tract", "cmedv"))]
X = data.matrix(X)
X = scale(X)
Y = as.vector(scale(BostonHousing2$cmedv))
k=0 # iteration 
est<-mylm_g(X,Y,matrix(0,ncol(X),1),0.1,10^(-14),10000) # estimation of beta
true<-lm(Y~X-1)$coefficients  # true value of beta
print(est)
print(true)
```

Here I choose the delta as 0.1, epsilon as 10^(-14), the maxitr as 10000. And the result of estimation of my function could be exactly the same as the result of estimation of linear regression without intercept. 

