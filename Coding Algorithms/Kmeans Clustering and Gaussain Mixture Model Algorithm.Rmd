---
title: 'STAT 542 / CS 598: Homework 5'
author: "Xinyi Song, xinyis8"
date: '10/28/2019'
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
---

```{r setup, include=FALSE}
  knitr::opts_chunk$set(include = TRUE)  # TRUE for solution; FALSE for questions set

  knitr::opts_chunk$set(echo = TRUE)
  knitr::opts_chunk$set(message = FALSE)
  knitr::opts_chunk$set(warning = FALSE)
  knitr::opts_chunk$set(fig.height = 6, fig.width = 8, out.width = '50%', fig.align = "center")
  options(width = 90)
```

```{css, echo=FALSE}
.solution {
background-color: #e6ffe6;
}
```


## Question 1 [50 Points] K-Means Clustering

Let's consider coding a K-means algorithm. Perform the following: 

  * Load the `zip.train` (handwritten digit recognition) data from the ElemStatLearn package, and the goal is to identify clusters of digits based on only the pixels. 
`Solution`

```{r}
library(ElemStatLearn)
data("zip.train")
```

  * [15 Points] Write your own code of k-means that iterates between two steps, and stop when the cluster membership does not change. 
    + updating the cluster means given the cluster membership
    + updating the cluster membership based on cluster means
    
`Solution`

```{r}
Mykmeans=function(K,data){
  #set.seed(15) # to keep stable results for convenience of writing comments 
  rand=sample(nrow(data),K,replace=FALSE)
  centroids=data[rand,]
  cluster=matrix(0,nrow(data),1)
  old_cluster=matrix(1,nrow(data),1)
  itr=1
  while(FALSE %in% unique(old_cluster==cluster)){
    old_cluster=cluster
    for (i in 1:nrow(data)){
      dist = matrix(0,nrow(centroids),1)
      for (c in 1:nrow(centroids)){
        dist[c]=sum((centroids[c,]-data[i,])^2)
      }
      cluster[i]=which.min(dist) # same min dist?
    }
    for (j in 1:nrow(centroids))
    {
      centroids[j,]=apply(data[which(cluster==j),],2,mean)
    }
    itr=itr+1
  }
  return(list(data=data.frame(data,cluster),centroids=centroids,itr=itr))
}
```

  * [10 Points] Perform your algorithm with one random initialization with $k = 5$
    + For this question, compare your cluster membership to the true digits. What are the most prevalent digits in each of your clusters?
    
`Solution`

```{r}
Mykmeans=function(K,data){
  set.seed(15) # to keep stable results for convenience of writing comments 
  rand=sample(nrow(data),K,replace=FALSE)
  centroids=data[rand,]
  cluster=matrix(0,nrow(data),1)
  old_cluster=matrix(1,nrow(data),1)
  itr=1
  while(FALSE %in% unique(old_cluster==cluster)){
    old_cluster=cluster
    for (i in 1:nrow(data)){
      dist = matrix(0,nrow(centroids),1)
      for (c in 1:nrow(centroids)){
        dist[c]=sum((centroids[c,]-data[i,])^2)
      }
      cluster[i]=which.min(dist) # same min dist?
    }
    for (j in 1:nrow(centroids))
    {
      centroids[j,]=apply(data[which(cluster==j),],2,mean)
    }
    itr=itr+1
  }
  return(list(data=data.frame(data,cluster),centroids=centroids,itr=itr))
}
lp=Mykmeans(5,zip.train[,-1])
table(zip.train[,1],lp$data[,257])
```

Comments: Here I set.seed(15) to keep the output stable and convenience of writing comments.

Based on the results above, we can see that for my cluster 1, the most prevelent digit is 7 and 9 also covers relatively large portion. For my cluster 2, the most prevelent digit is 0. For my cluster 3, the most prevelent digit is 1.For my cluster 4, the most prevelent digit is 3.For my cluster 5, the most prevelent digit is 6 and 2 alco covers a relatively large portion.

  * [10 Points] Perform your algorithm with 10 independent initiations with $k = 5$ and record the best
    + For this question, plot your clustering results on a two-dimensional plot, where the two axis are the first two principle components of your data

`Solution`

```{r}
sed<-c(521,5,155,42,88,9,447,49,1,4,9999)
Mykmeans=function(u,K,data){
  set.seed(u)
  rand=sample(nrow(data),K,replace=FALSE)
  centroids=data[rand,]
  cluster=matrix(0,nrow(data),1)
  old_cluster=matrix(1,nrow(data),1)
  itr=1
  while(FALSE %in% unique(old_cluster==cluster)){
    old_cluster=cluster
    for (i in 1:nrow(data)){
      dist = matrix(0,nrow(centroids),1)
      for (c in 1:nrow(centroids)){
        dist[c]=sum((centroids[c,]-data[i,])^2)
      }
      cluster[i]=which.min(dist) # same min dist?
    }
    for (j in 1:nrow(centroids))
    {
      centroids[j,]=apply(data[which(cluster==j),],2,mean)
    }
    itr=itr+1
  }
  return(list(data=data.frame(data,cluster),centroids=centroids,itr=itr))
}
# initial 1
init1 = Mykmeans(sed[1],5,zip.train[,-1])
result=init1$data
cent=init1$centroids
dist=matrix(0,5,1)
for (k in 1:5){
  cluster=result[which(result$cluster==k),] 
  cl=cluster[,-257]
  h=t(as.matrix(cent[k,]))
  t=apply(cl,1,function(x) sum((x-h)^2))
  dist[k]=sum(t)
}
result_init1=sum(dist)
# intial 2
init2 = Mykmeans(sed[2],5,zip.train[,-1])
result=init2$data
cent=init2$centroids
dist=matrix(0,5,1)
for (k in 1:5){
  cluster=result[which(result$cluster==k),] 
  cl=cluster[,-257]
  h=t(as.matrix(cent[k,]))
  t=apply(cl,1,function(x) sum((x-h)^2))
  dist[k]=sum(t)
}
result_init2=sum(dist)
# initial 3
init3 = Mykmeans(sed[3],5,zip.train[,-1])
result=init3$data
cent=init3$centroids
dist=matrix(0,5,1)
for (k in 1:5){
cluster=result[which(result$cluster==k),] 
cl=cluster[,-257]
h=t(as.matrix(cent[k,]))
t=apply(cl,1,function(x) sum((x-h)^2))
dist[k]=sum(t)
}
result_init3=sum(dist)
# initial 4
init4 = Mykmeans(sed[4],5,zip.train[,-1])
result=init4$data
cent=init4$centroids
dist=matrix(0,5,1)
for (k in 1:5){
  cluster=result[which(result$cluster==k),] 
  cl=cluster[,-257]
  h=t(as.matrix(cent[k,]))
  t=apply(cl,1,function(x) sum((x-h)^2))
  dist[k]=sum(t)
}
result_init4=sum(dist)
# initial 5 
init5 = Mykmeans(sed[5],5,zip.train[,-1])
result=init5$data
cent=init5$centroids
dist=matrix(0,5,1)
for (k in 1:5){
  cluster=result[which(result$cluster==k),] 
  cl=cluster[,-257]
  h=t(as.matrix(cent[k,]))
  t=apply(cl,1,function(x) sum((x-h)^2))
  dist[k]=sum(t)
}
result_init5=sum(dist)
# initial 6
init6 = Mykmeans(sed[6],5,zip.train[,-1])
result=init6$data
cent=init6$centroids
dist=matrix(0,5,1)
for (k in 1:5){
  cluster=result[which(result$cluster==k),] 
  cl=cluster[,-257]
  h=t(as.matrix(cent[k,]))
  t=apply(cl,1,function(x) sum((x-h)^2))
  dist[k]=sum(t)
}
result_init6=sum(dist)
# initial 7
init7 = Mykmeans(sed[7],5,zip.train[,-1])
result=init7$data
cent=init7$centroids
dist=matrix(0,5,1)
for (k in 1:5){
  cluster=result[which(result$cluster==k),] 
  cl=cluster[,-257]
  h=t(as.matrix(cent[k,]))
  t=apply(cl,1,function(x) sum((x-h)^2))
  dist[k]=sum(t)
}
result_init7=sum(dist)
# initial 8
init8 = Mykmeans(sed[8],5,zip.train[,-1])
result=init8$data
cent=init8$centroids
dist=matrix(0,5,1)
for (k in 1:5){
  cluster=result[which(result$cluster==k),] 
  cl=cluster[,-257]
  h=t(as.matrix(cent[k,]))
  t=apply(cl,1,function(x) sum((x-h)^2))
  dist[k]=sum(t)
}
result_init8=sum(dist)
# inital 9
init9 = Mykmeans(sed[9],5,zip.train[,-1])
result=init9$data
cent=init9$centroids
dist=matrix(0,5,1)
for (k in 1:5){
  cluster=result[which(result$cluster==k),] 
  cl=cluster[,-257]
  h=t(as.matrix(cent[k,]))
  t=apply(cl,1,function(x) sum((x-h)^2))
  dist[k]=sum(t)
}
result_init9=sum(dist)
# initial 10
init10 = Mykmeans(sed[10],5,zip.train[,-1])
result=init10$data
cent=init10$centroids
dist=matrix(0,5,1)
for (k in 1:5){
  cluster=result[which(result$cluster==k),] 
  cl=cluster[,-257]
  h=t(as.matrix(cent[k,]))
  t=apply(cl,1,function(x) sum((x-h)^2))
  dist[k]=sum(t)
}
result_init10=sum(dist)
result_total=c(result_init1,result_init2,result_init3,result_init4,result_init5,result_init6,result_init7,result_init8,result_init9,result_init10)
print(result_total)
which.min(result_total)
# the best intial whose result has smallest within cluster distance among the ten trials
print(which.min(result_total))
# the smallest within cluster distance among the ten trials
print(result_total[which.min(result_total)])
```
  
```{r}
pcafit <- princomp(zip.train[,-257])
PC1=pcafit$scores[,1]
PC2=pcafit$scores[,2]
# initial 9 has the smallest within cluster distance among the ten trials
best=Mykmeans(sed[which.min(result_total)],5,zip.train[,-1])
plot(PC1,PC2,col =best$data[,257],pch = 1,main='PCA')
```


  * [15 Points] Compare the clustering results from the above two questions with the built-in `kmeans()` function in R. Use tables/figures to demonstrate your results and comment on your findings.
  
`Solution`

```{r}
pac.kmean <- kmeans(zip.train[,-257], centers = 5, nstart = 20)
dist.pac <- sum(pac.kmean$withinss)
print(dist.pac)
```

Comments:



## Question 2 [50 Points] Two-dimensional Gaussian Mixture Model

We consider an example of the EM algorithm, which fits a Gaussian mixture model to the Old Faithful eruption data. For a demonstration of this problem, see the figure provided on [Wikipedia](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm). As the end result, we will obtain the distribution parameters of the two underlying distributions. We consider the problem as follows. For this question, you are allowed to use packages that calculate the densities of normal distributions. 

* We use both variables `eruptions` and `waiting`. We assume that the underlying distributions given the unobserved latent variables are both two-dimensional normal: $N(\mu_1, \Sigma_1)$ and $N(\mu_2, \Sigma_2)$, respectively, while $\mu_1$, $\Sigma_1$, $\mu_2$, and $\Sigma_2$ are unknow parameters that we need to solve.
* We assume that the unobserved latent variables (that indicate the membership) follow i.i.d. Bernoulli distribution, with parameter $p$.
* Based on the logic of an EM algorithm, we will first initiate some values of the parameters in the normal distribution. I provided a choice of them, and the normal density plots based on the initial values.
* Your goal is to write the EM algorithm that progressively updates the parameters and the latent variable distribution parameter. Eventually, we will reach a stable model fitting result that approximate the two underlying distributions, as demonstrated on the Wikipedia page. Choose a reasonable stopping criterion. To demonstrate your results, you should provide at least the following information. 
  + The distribution parameters $p$, $\mu_1$, $\Sigma_1$, $\mu_2$, and $\Sigma_2$
  + A histogram of the underlying probabilities of the latent variables
  + Plot the normal densities at the 2nd, 3rd, 4th and the final iteration of your algorithm
* Now, experiment a very different initial value of the parameters and rerun the algorithm. Comment on the efficiency and convergence speed of this algorithm.  

`Solution`

```{r}
library(mixtools)
library(mvtnorm)
addellipse <- function(mu, Sigma, ...)
{
  ellipse(mu, Sigma, alpha = .05, lwd = 1, ...)
  ellipse(mu, Sigma, alpha = .25, lwd = 2, ...)
}
Estep=function(data,G,para){
  pr = para$prob
  mu = para$mean
  Sigma=para$Sigma
  n = nrow(data)
  d1 = pr[1]*dmvnorm(faithful,mu[,1],Sigma[,,1])/(pr[1]*dmvnorm(faithful,mu[,1],Sigma[,,1])+pr[2]*dmvnorm(faithful,mu[,2],Sigma[,,2]))
  d2 = pr[2]*dmvnorm(faithful,mu[,2],Sigma[,,2])/(pr[1]*dmvnorm(faithful,mu[,1],Sigma[,,1])+pr[2]*dmvnorm(faithful,mu[,2],Sigma[,,2]))
  post.prob=cbind(as.matrix(d1),as.matrix(d2))
  return(post.prob)
}
Mstep <- function (data, G, para, post.prob) {
  pi=apply(post.prob,2,mean)
  mean<-matrix(0,ncol(data),G)
  for (i in 1:2){
    mean[,i]=apply(data,2,function(x) t(x)%*%post.prob[,i]/sum(post.prob[,i]))
  }
  bl=array(0,dim=c(2,2,2))
  for (k in 1:2){
    tmp=0
    for (m in 1:nrow(faithful)){
      tmp=t(as.matrix(faithful[m,]-t(as.matrix(mean[,k]))))%*%(as.matrix(faithful[m,]-t(as.matrix(mean[,k]))))*post.prob[m,k]+tmp
    }
    tmmp=tmp/sum(post.prob[,k])
    bl[,,k]=tmmp
  }
  para$Sigma=bl
  para$mean=mean
  para$prob=pi
  return(para)
}
myEM <- function (data, G, para,threshold) {
  itr=1
  log=10
  log=c(log,0)
  while(abs(log[itr+1]-log[itr])>=threshold){
    post.prob <- Estep (data,G,para )
    para <- Mstep (data,G,para,post.prob)
    log = c(log, sum(post.prob[,1]*log(dmvnorm(faithful,para$mean[,1],para$Sigma[,,1]))+post.prob[,2]*log(dmvnorm(faithful,para$mean[,2],para$Sigma[,,2])))+sum(post.prob[,1]*log(para$prob[1])+(post.prob[,2])*log(para$prob[2])))
    itr=itr+1
    if ((itr==3)){ # 2nd iteration
      plot(faithful)
      addellipse(para$mean[,1],para$Sigma[,,1], col = "darkorange")
      addellipse(para$mean[,2],para$Sigma[,,2], col = "deepskyblue")
    }
    if ((itr==4)){ # 3rd iteration
      plot(faithful)
      addellipse(para$mean[,1],para$Sigma[,,1], col = "darkorange")
      addellipse(para$mean[,2],para$Sigma[,,2], col = "deepskyblue")
    }
    if ((itr==5)){ # 4th iteration
      plot(faithful)
      addellipse(para$mean[,1],para$Sigma[,,1], col = "darkorange")
      addellipse(para$mean[,2],para$Sigma[,,2], col = "deepskyblue")
    }
  }
  para$itr=itr-1
  para$post.prob=post.prob
  return(para)
}
faithful = read.table("https://teazrq.github.io/stat542/data//faithful.txt")
# the parameters
mu1 = c(3, 80)
mu2 = c(3.5, 60)
Sigma1 = matrix(c(0.1, 0, 0, 10), 2, 2) 
Sigma2 = matrix(c(0.1, 0, 0, 50), 2, 2)
Sigma <- array(c(Sigma1,Sigma2),dim = c(2,2,2))
para=list()
para$mean=cbind(as.matrix(mu1),as.matrix(mu2))
para$Sigma=Sigma
para$prob=c(0.5,0.5)
z1 <- Sys.time()
y=myEM(data=faithful,2,para, 10^(-5))
z2 <- Sys.time()
print(z2-z1)
print(y$itr)
print(y$mean)
print(y$Sigma)
print(y$prob)
# Plot final iteration
plot(faithful)
addellipse(y$mean[,1],y$Sigma[,,1], col = "darkorange")
addellipse(y$mean[,2],y$Sigma[,,2], col = "deepskyblue")
# histogram of latent variable 
latent_variable_1=y$post.prob[,1]
latent_variable_2=y$post.prob[,2]
hist(latent_variable_1,breaks=10)
hist(latent_variable_2)
```

Then, I tried a different initial value of prob: (0.01,0.99)

```{r,warning=FALSE}
faithful = read.table("https://teazrq.github.io/stat542/data//faithful.txt")
# the parameters
mu1 = c(3, 80)
mu2 = c(3.5, 60)
Sigma1 = matrix(c(0.1, 0, 0, 10), 2, 2) 
Sigma2 = matrix(c(0.1, 0, 0, 50), 2, 2)
Sigma <- array(c(Sigma1,Sigma2),dim = c(2,2,2))
para=list()
para$mean=cbind(as.matrix(mu1),as.matrix(mu2))
para$Sigma=Sigma
para$prob=c(0.01,0.99) # different initial value 
t1 <- Sys.time()
z=myEM(data=faithful,2,para, 10^(-5))
t2 <- Sys.time()
print(t2-t1)
itr=z$itr
print(itr)
print(z$mean)
print(z$Sigma)
print(z$prob)
plot(faithful)
addellipse(z$mean[,1],z$Sigma[,,1], col = "darkorange")
addellipse(z$mean[,2],z$Sigma[,,2], col = "deepskyblue")
# histogram of latent variable 
latent_variable_1=z$post.prob[,1]
latent_variable_2=z$post.prob[,2]
hist(latent_variable_1,breaks=10)
hist(latent_variable_2)
```

Comments: 

For my EM algorithm, to reach a stable model fitting result, the stopping criterion that I choose is the difference of loglikelihoodwith parameters that being updated between two iterations is less than 10^(-5). Also, 10^(-5) here is regarded as a parameter of threshold, in my EM algorithm function, we can change the threshold depending the real situation.

Based on the result with a differnt and more extreme initial value prob: (0.01, 0.09), I can see that when I choose an more extreme initial value, the iteration times become larger and the convergent time becomes longer, which leads to a relatively lower efficiency.And these things could be reflected from the total iteration times and the real time that the function runs. 

And it will finally converge regardless of initial value and based on my threshold 10^(-5), the final result of EM algorithm will be same with reasonable stopping criterion chosen.

## Reference 

EM Algorithm: https://www.cs.ubc.ca/~murphyk/Teaching/CS340-Fall06/reading/mixtureModels.pdf