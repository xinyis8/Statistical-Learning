---
title: "HW2_xinyis8"
author: "Xinyi Song"
date: "9/16/2019"
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question 1 [20 Points] Linear Model Selection

We will use the Boston Housing data again. This time, we do not scale the covariate. We will still remove medv, town and tract from the data and use cmedv as the outcome. If you do not use R, you can download a '.csv' file from the course website.

```{r}
library(mlbench)
data(BostonHousing2)
BH = BostonHousing2[, !(colnames(BostonHousing2) %in% c("medv", "town", "tract"))]
full.model = lm(cmedv~., data = BH)
summary(full.model)
```
Answer the following questions:

a. [5 Points] Report the most significant variable from this full model with all features.

`Solution`

Based on the output of the full model, I can see that rm and lstat have the smallest P-value, but lstat's t statistics is more extreme than that of rm, thus lstat is the most significant variable from this full model with all features.

b. [5 Points] Starting from this full model, use stepwise regression with both forward and backward and BIC criterion to select the best model. Which variables are removed from the full model?

`Solution`


```{r}
full.model = lm(cmedv~., data = BH)
n = dim(BH)[1]
summary(full.model)
# Forward Selection with BIC
stepBIC.for = step(full.model, direction="forward", k=log(n))    
var.BIC.for = attr(stepBIC.for$terms, "term.labels")
# Backward Selection with BIC
stepBIC.back = step(full.model, direction="backward", k=log(n))    
var.BIC.back = attr(stepBIC.back$terms, "term.labels")
full.var = attr(full.model$terms, "term.labels")
remove.back.bic<-full.var[-which(full.var%in%var.BIC.back)]
remove.for.bic<-full.var[-which(full.var%in%var.BIC.for)]
print(remove.back.bic)
print(remove.for.bic)
```
Based on the output above, we can see that as for forward selection with BIC, the result of its model selection will be exactly the same as full model does. While for backward selection with BIC, it removes four variables: lon,lat, indus and age.


c. [5 Points] Starting from this full model, use the best subset selection and list the best model of each model size.

`Solution`

```{r}
library(leaps)
ret.full <- regsubsets(cmedv~., data = BH,nvmax=dim(BH)-1)
sum.ret.full <- summary(ret.full)
print(sum.ret.full$outmat)
rss<-as.vector(sum.ret.full$rss)
```
Comments:

Based on the output above, we can find that for model size from 1 to 15, the variables selected are shown as 'TRUE'. For instance, when the model size is two, rm and lstat are selected and form the best subset selection.

d. [5 Points] Use the Cp criterion to select the best model from part c). Which variables are removed from the full model? What is the most significant variable?

`Solution`

```{r}
cp<-as.vector(sum.ret.full$cp)
print(which(cp==cp[which.min(cp)])) # Model with lowest Cp Value
print(sum.ret.full$outmat)
model_eleven = lm(cmedv~crim+zn+chas+nox+rm+dis+rad+tax+ptratio+b+lstat, data = BH)
summary(model_eleven)
```

Based on the output above, we can find that according to Cp criterion, model 11 is best with smallest cp among the all 15 models being output. And the variables including lon, lat,indus and age are removed in this model. 

Based on the output of this model, we can see that rm and lstat have smallest P-values among all the variables, similarly, lstat't statistics is more extreme than that of rm, thus lstat is the most significant variable.

## Question 2

For this question, we will write our own Lasso code. You are not allowed to use any built-in package that already implements Lasso. First, we will generate simulated data. Here, only X1, X2 and X3 are important, and we will not consider the intercept term.
As we already know, coordinate descent is an efficient approach for solving Lasso. The algorithm works by updating one parameter at a time, and loop around all parameters until convergence.

a. [10 Points] Hence, we need first to write a function that updates just one parameter, which is also known as the soft-thresholding function. Construct the function in the form of soft_th <- function(b, lambda), where b is a number that represents the one-dimensional linear regression solution, and lambda is the penalty level. The function should output a scaler, which is the minimizer of

$$
(x-b)^2+\lambda |b|
$$

```{r}
library(MASS) 
set.seed(1)
n = 200
p = 200
# generate data
V = matrix(0.2, p, p)
diag(V) = 1
X = as.matrix(mvrnorm(n, mu = rep(0, p), Sigma = V))
y = X[, 1] + 0.5*X[, 2] + 0.25*X[, 3] + rnorm(n)
# we will use a scaled version
x = scale(X) 
y = scale(y)
#####################
soft_th = function(b, lam){
  y <- ifelse(abs(b)<=(1/2*lam), 0, ifelse(b>lam/2, b-lam/2, b+lam/2))
  return(y)
}
```

b. [10 Points] Now lets pretend that at an iteration, the current parameter $\mathbf{\beta}$ value is given below (as beta_old, i.$\mathbf{\beta^{old}}$). Apply the above soft-thresholding function to update all p parameters sequencially one by one to complete one "loop" of the updating scheme. Please note that we use the Gauss- Seidel style coordinate descent, in which the update of the next parameter is based on the new values of previous entries. Hence, each time a parameter is updated, you should re-calculate the residual

$$
\mathbf{r}=y-\mathbf{X}^\mathrm{T} \mathrm{\beta}
$$

so that the next parameter update reflects this change. After completing this one enrire loop, print out the first 3 observations of r and the nonzero entries in the updated $\beta^{new}$ vector. For this question,use lambda = 0.7 and beta_old = rep(0, p)

`Solution`
```{r}
set.seed(1)
library(MASS) 
set.seed(1)
n = 200
p = 200
# generate data
V = matrix(0.2, p, p)
diag(V) = 1
X = as.matrix(mvrnorm(n, mu = rep(0, p), Sigma = V))
y = X[, 1] + 0.5*X[, 2] + 0.25*X[, 3] + rnorm(n)
# we will use a scaled version
x = scale(X)
y = scale(y)
n=200
b=rep(0,p)
R = matrix(0,p,3) # save residuals
r= y-x%*%b
lambda=0.7
R=matrix(0,p,3)
 for(j in 1:p){
    r <- r+ x[,j]*b[j]
    if(j<=3){
      R[,j]=r
    }
    b[j]=soft_th(((t(x[,j])%*%r)/(t(x[,j])%*%x[,j])),(n*lambda/sum(t(x[,j])*x[,j])))
    r <- r - x[,j]*b[j]
 }
print(R)
print(b[which(b!=0)])

```

c. [25 Points] Now, let us finish the entire Lasso algorithm. We will write a function myLasso(X, y, lambda, tol, maxitr). Set the tolerance level tol = 1e-5, and maxitr = 100 as the default value. Use the 'one loop' code that you just wrote in the previous question, and integrate that into a grand for-loop that will continue updating the parameters up to maxitr runs. Check your parameter updates once in this grand loop and stop the algorithm once the l1 distance between $\beta_{new}$ and $\beta_{old}$ is smaller than tol. Use beta_old = rep(0, p) as the initial value, and lambda = 0.3. After the algorithm converges, report the following: i) the number of iterations took; ii) the nonzero entries in the final beta parameter estimate, and iii) the first three observations of the residual. Please write your algorithm as efficient as possible.

`Solution`
```{r,warnings=FALSE}
#####################
soft_th = function(b, lam){
  y <- ifelse(abs(b)<=(1/2*lam), 0, ifelse(b>lam/2, b-lam/2, b+lam/2))
  return(y)
}
myLasso=function(x, y,lambda, tol, maxitr){
# Initial values
b <- rep(0,p)
c<-rep(0.1,p)
r <- y-x%*%b
itr=1
# Coordiante descent
h=matrix(1,p,1)
residual=matrix(0.1,p,3)
while(itr<=maxitr){
  if(sum(abs(b-h))>tol){
    for(j in 1:p){
    h[j]=b[j] # Here, h is for beta_old and b is for beta_new
    r <- r+ x[,j]*b[j]
    b[j]=soft_th(((t(x[,j])%*%r)/(t(x[,j])%*%x[,j])),(n*lambda/sum(t(x[,j])*x[,j])))
    r <- r - x[,j]*b[j]
    if ((itr==1) & (j<=3)){
      residual[,j]=r
    }
    else{
      residual=residual
    }
    }
    itr=itr+1
}
  else{
    break
  }
}
b_eff<-cbind(which(b!=0),b[which(b!=0)])
return(list(itr,b,b_eff,residual))
}
t<-myLasso(x,y,0.3,1e-5,100)
itr<-t[[1]]
# Nonzero entries in the final beta parameter estimate
b_eff<-t[[3]]
re<-t[[4]]
# Number of iterations it took
print(itr)
# Nonzero entries in the final beta parameter estimate
print(b_eff)
# the first three observations of the residual
print(re)
```

d. [5 Points] Now we have our own Lasso function, let's check the result and compare it with the glmnet package. Note that for the glmnet package, their lambda should be set as half of ours. Comment on the accuracy of the algorithm that we wrote. Please note that the distance of the two solutions should not be larger than 0.005.

`Solution`
```{r}
library(glmnet)
t<-myLasso(x,y,0.3,10^(-5),100)
lasso.fit = glmnet(x, y, alpha = 1, lambda = 0.3/2,standardize = TRUE) 
beta_mylasso<-as.matrix(t[[2]])
# check the result and compare it with the glmnet package
result=cbind(coef(lasso.fit)[-1],beta_mylasso)
print(result)
dist = sum(abs(coef(lasso.fit)[-1]-beta_mylasso))
print(dist)
```

Comments: Based on the result above, we can find that the estimation of myLasso is very close to that of glmnet, the distance of the two solutions is about 0.001104, which is far less than 0.005. The result is accurate.

## Question 3 (30 Points) Cross-Validation for Model Selection

We will use the Walmart Sales data provided on Kaggle. For this question, we will use only the Train.csv file. The file is also available at here.
a. [10 Points] Do the following to process the data:

* Read data into R
* Convert character variables into factors
* Remove Item_Identifier
* Further convert all factors into dummy variables

`Solution`

```{r}
library(caret)
library(glmnet)
# Read data into R
traindata<-read.csv("~/Desktop/2019 Fall/STAT 542/hw2/walmart-sales/Train.csv")
library(dplyr)
#Convert character variables into factors
f <- mutate_if(traindata, is.character, as.factor) 
# Remove Item_Identifier
train<-as.data.frame(f[,-which(colnames(traindata)=="Item_Identifier")])
# convert factors into dummies
WalMartData <- model.matrix( ~ . -1, data = traindata[, -c(1, 7)])
```




b. [20 Points] Use all variables to model the outcome Item_Outlet_Sales in its log scale. First, we randomly split the data into two parts with equal size. Make sure that you set a random seed so that the result can be replicated. Treat one as the training data, and the other one as the testing data. For the training data, perform the following:

* Use cross-validation to select the best Lasso model. Consider both lambda.min and lambda.1se. Provide additional information to summarize the model fitting result

* Use cross-validation to select the best Ridge model. Consider both lambda.min and lambda.1se. Provide additional information to summarize the model fitting result

* Test these four models on the testing data and report and compare the prediction accuracy

`Solution`

```{r}
set.seed(1)
sale_log<-log(WalMartData[,"Item_Outlet_Sales"])
WalMartData=WalMartData[,-dim(WalMartData)[2]]
id = sample(1:nrow(WalMartData),(nrow(WalMartData)+1)/2,replace=FALSE)
train_data=WalMartData[id,]
train_y=sale_log[id]
test_data=WalMartData[-id,]
test_y=sale_log[-id]
# Lasso
mycv = cv.glmnet(x=as.matrix(train_data), y=as.matrix(train_y),
                 nfolds = 10, alpha=1)
myfit_lasso_min=glmnet(x=as.matrix(train_data), y=as.matrix(train_y), lambda = mycv$lambda.min, alpha=1)
myfit_lasso_1se=glmnet(x=as.matrix(train_data), y=as.matrix(train_y), lambda = mycv$lambda.1se, alpha=1)
coef(myfit_lasso_min, s = "lambda.min")
coef(myfit_lasso_1se, s = "lambda.1se")
```

Comment:

Here,lambda.min is the value of $\lambda_{min}$ that gives minimum mean cross-validated error. The other $\lambda_{1se}$ saved is lambda.1se , which gives the most regularized model such that error is within one standard error of the minimum. Here, for lasso regression, the min_lambda is 0.002189116 while lambda_1se is 0.03250768.

For Lasso Regression, here, we can see that comparing with lambda.min, lambda.1se tunes more variables to zero, which makes the model easier to be interpreted. Also, for the common variables left in the two models with lambda.min and lambda.1se, the lambda.1se tunes more. In other words, the lambda.1se gives smaller magnitude of coefficients in the remaining variables.

```{r}
# Ridge
set.seed(1)
mycv = cv.glmnet(x=as.matrix(train_data), y=as.matrix(train_y),
                 nfolds = 10, alpha=0)
myfit_ridge_min=glmnet(x=as.matrix(train_data), y=as.matrix(train_y), lambda = mycv$lambda.min, alpha=0)
myfit_ridge_1se=glmnet(x=as.matrix(train_data), y=as.matrix(train_y), lambda = mycv$lambda.1se, alpha=0)
coef(myfit_ridge_min, s = "lambda.min")
coef(myfit_ridge_1se, s = "lambda.1se")
```
Comment: For ridge regression, the lambda.min is 0.05297943 while lambda.1se is 0.09258305. Based on the output above, we can see that comparing with lambda.min, lambda.1se tunes the variables more, which indicates that the magnitude of coefficients is smaller than that of model with lambda.min. For ridge regression, they cannot perform variable selection, thus they keep all the variables in the whole mode. Other things being equal, larger lambda tunes the parameters more. 

```{r}
# Lasso Performance Evaluation
pred_lasso_min=predict(myfit_lasso_min,test_data, type = "response")
pred_lasso_1se=predict(myfit_lasso_1se,test_data, type = "response")
MSE_lasso_min=mean((pred_lasso_min-test_y)^2)
MSE_lasso_1se=mean((pred_lasso_1se-test_y)^2)
print(cbind(MSE_lasso_min,MSE_lasso_1se))
# Ridge Performance Evaluation 
pred_ridge_min=predict(myfit_ridge_min,test_data, type = "response")
pred_ridge_1se=predict(myfit_ridge_1se,test_data, type = "response")
MSE_ridge_min=mean((pred_ridge_min-test_y)^2)
MSE_ridge_1se=mean((pred_ridge_1se-test_y)^2)
print(cbind(MSE_ridge_min,MSE_ridge_1se))
```

```{r}
####Figure
fit_lasso = cv.glmnet(x=as.matrix(train_data),y=as.matrix(train_y) , nfolds = 10,alpha=1)
par(mfrow = c(1, 2))
plot(fit_lasso)
plot(fit_lasso$glmnet.fit, "lambda")
```

Comments: Here, I use mean square error(MSE) to evaluate the performance of model. I calculate the MSE of log(sale_price) between the true value and model prediction. Based on the results above, we can see that for lasso regression, the MSE of lasso with labmda.min is smaller than that of lasso with lambda.1se. However, there is not large differences between these two MSEs. At this time, under certain circumstances, I prefer lambda.1se since it tunes more variables to zero with better characteristics of interpretability and the mse of it is not as much different as that of model with lambda.min.

Besides, for ridge regression, also, the model with lambda.min performs better than lambda.1se in terms of MSE. But both of them cannot perform variable selection due to l2 norm properties, thus at this time, under certain circumstances, I think that I prefer the ridge regression with lambda.min.

Also, I plot two figures. One shows the relationship between log(lambda) and MSE, we can see for the two vertial dotted lines, there is not much difference in their MSEs, which verifies what I found in lasso regression with lambda.min and lambda.1se.

For the right one, we can see that other things being equal, larger lambda values tend to tune the parameters more, and from the figure above, we can see that when log(lambda)>-1,it almost tunes every coefficient into zero, and this also verifies what we have found.

## Bonus Question 

You probably noticed that our results from myLasso function are not exactly the same as glmnet. What are
the possible reasons that cause this difference? Try to obtain a closer solution with some minor twists. You
should be able to obtain a solution that is at the 1e-5 level distance from glmnet.

`Solution` 

After reading the package glmnet, I know that glmnet standardizes the data using a different definition of "standard deviation", which is divided by n, not (n-1) as defined in R. Thus, instead, I follow the scaling method of glmnet to scale my data and then plug into my lasso and glmnet function.

```{r}
library(MASS)
set.seed(1)
n = 200
p = 200
# generate data
V = matrix(0.2, p, p)
diag(V) = 1
X = as.matrix(mvrnorm(n, mu = rep(0, p), Sigma = V))
Y= X[, 1] + 0.5*X[, 2] + 0.25*X[, 3] + rnorm(n)
n = length(y)
X.means = apply(X, 2, mean)
X.sd = apply(X, 2, sd)*sqrt((n-1)/n)
X_scale=matrix(0,200,200)
for (i in 1:p){
  X_scale[,i]=(X[,i]-X.means[i])/X.sd[i]
}
Y.means = mean(Y)
Y.sd = sd(Y)*sqrt((n-1)/n)
Y_scale = (Y - Y.means)/Y.sd 
library(glmnet)
lasso.fit = glmnet(X_scale, Y_scale, alpha = 1,lambda=0.3/2)
t<-myLasso(X_scale,Y_scale,0.3,10^(-5),100)
dist=sum(abs(as.matrix(t[[2]])-coef(lasso.fit)[-1]))
print(sum(abs(as.matrix(t[[2]])-coef(lasso.fit)[-1])))

```
Here I reduce the l1 distance between the estimation of glmnet and estimation of mylasso function to 1.567451e-05.